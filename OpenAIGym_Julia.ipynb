{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code is just trying to get the julia wrapper at https://github.com/JuliaML/OpenAIGym.jl to work . I had to make few changes and also the example on that repo doesn't work - the example at the bottom of this file works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module OpenAIGym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIGym"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " __precompile__()\n",
    "\n",
    "module OpenAIGym\n",
    "\n",
    "using PyCall\n",
    "using Reexport\n",
    "@reexport using Reinforce\n",
    "import Reinforce:\n",
    "    MouseAction, MouseActionSet,\n",
    "    KeyboardAction, KeyboardActionSet\n",
    "\n",
    "abstract type AbstractGymEnv<:AbstractEnvironment end\n",
    "export\n",
    "    gym,\n",
    "    GymEnv\n",
    "\n",
    "const _py_envs = Dict{String,Any}()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "abstract type AbstractGymEnv<:AbstractEnvironment end\n",
    "\n",
    "\n",
    "\"A simple wrapper around the OpenAI gym environments to add to the Reinforce framework\"\n",
    "type GymEnv <: AbstractGymEnv\n",
    "    name::String\n",
    "    pyenv  # the python \"env\" object\n",
    "    state\n",
    "    reward::Float64\n",
    "    actions::AbstractSet\n",
    "    done::Bool\n",
    "    info::Dict\n",
    "    GymEnv(name,pyenv) = new(name,pyenv)\n",
    "end\n",
    "GymEnv(name) = gym(name)\n",
    "\n",
    "function Reinforce.reset!(env::GymEnv)\n",
    "    env.state = env.pyenv[:reset]()\n",
    "    env.reward = 0.0\n",
    "    env.actions = actions(env, nothing)\n",
    "    env.done = false\n",
    "end\n",
    "\n",
    "\"A simple wrapper around the OpenAI gym environments to add to the Reinforce framework\"\n",
    "type UniverseEnv <: AbstractGymEnv\n",
    "    name::String\n",
    "    pyenv  # the python \"env\" object\n",
    "    state\n",
    "    reward\n",
    "    actions::AbstractSet\n",
    "    done\n",
    "    info::Dict\n",
    "    UniverseEnv(name,pyenv) = new(name,pyenv)\n",
    "end\n",
    "UniverseEnv(name) = gym(name)\n",
    "\n",
    "function Reinforce.reset!(env::UniverseEnv)\n",
    "    env.state = env.pyenv[:reset]()\n",
    "    env.reward = [0.0]\n",
    "    env.actions = actions(env, nothing)\n",
    "    env.done = [false]\n",
    "end\n",
    "\n",
    "function gym(name::AbstractString)\n",
    "    env = if name in (\"Soccer-v0\", \"SoccerEmptyGoal-v0\")\n",
    "        @pyimport gym_soccer\n",
    "        get!(_py_envs, name) do\n",
    "            GymEnv(name, pygym[:make](name))\n",
    "        end\n",
    "    elseif split(name, \".\")[1] in (\"flashgames\", \"wob\")\n",
    "        @pyimport universe\n",
    "        @pyimport universe.wrappers as wrappers\n",
    "        if !isdefined(OpenAIGym, :vnc_event)\n",
    "            global const vnc_event = PyCall.pywrap(PyCall.pyimport(\"universe.spaces.vnc_event\"))\n",
    "        end\n",
    "        get!(_py_envs, name) do\n",
    "            pyenv = wrappers.SafeActionSpace(pygym[:make](name))\n",
    "            pyenv[:configure](remotes=1)  # automatically creates a local docker container\n",
    "            # pyenv[:configure](remotes=\"vnc://localhost:5900+15900\")\n",
    "            o = UniverseEnv(name, pyenv)\n",
    "            # finalizer(o,  o.pyenv[:close]())\n",
    "            sleep(2)\n",
    "            o\n",
    "        end\n",
    "    else\n",
    "        GymEnv(name, pygym[:make](name))\n",
    "    end\n",
    "    reset!(env)\n",
    "    env\n",
    "end\n",
    "\n",
    "function actionset(A::PyObject)\n",
    "    if haskey(A, :n)\n",
    "        # choose from n actions\n",
    "        DiscreteSet(0:A[:n]-1)\n",
    "    elseif haskey(A, :spaces)\n",
    "        # a tuple of action sets\n",
    "        sets = [actionset(a) for a in A[:spaces]]\n",
    "        TupleSet(sets...)\n",
    "    elseif haskey(A, :high)\n",
    "        # continuous interval\n",
    "        IntervalSet{Vector{Float64}}(A[:low], A[:high])\n",
    "        # if A[:shape] == (1,)  # for now we only support 1-length vectors\n",
    "        #     IntervalSet{Float64}(A[:low][1], A[:high][1])\n",
    "        # else\n",
    "        #     # @show A[:shape]\n",
    "        #     lo,hi = A[:low], A[:high]\n",
    "        #     # error(\"Unsupported shape for IntervalSet: $(A[:shape])\")\n",
    "        #     [IntervalSet{Float64}(lo[i], hi[i]) for i=1:length(lo)]\n",
    "        # end\n",
    "    elseif haskey(A, :buttonmasks)\n",
    "        # assumed VNC actions... keys to press, buttons to mask, and screen position\n",
    "        # keyboard = DiscreteSet(A[:keys])\n",
    "        keyboard = KeyboardActionSet(A[:keys])\n",
    "        buttons = DiscreteSet(Int[bm for bm in A[:buttonmasks]])\n",
    "        width,height = A[:screen_shape]\n",
    "        mouse = MouseActionSet(width, height, buttons)\n",
    "        TupleSet(keyboard, mouse)\n",
    "    elseif haskey(A, :actions)\n",
    "        # Hardcoded\n",
    "        TupleSet(DiscreteSet(A[:actions]))\n",
    "    else\n",
    "        @show A\n",
    "        @show keys(A)\n",
    "        error(\"Unknown actionset type: $A\")\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function Reinforce.actions(env::AbstractGymEnv, s′)\n",
    "    actionset(env.pyenv[:action_space])\n",
    "end\n",
    "\n",
    "pyaction(a::Vector) = Any[pyaction(ai) for ai=a]\n",
    "pyaction(a::KeyboardAction) = Any[a.key]\n",
    "pyaction(a::MouseAction) = Any[vnc_event.PointerEvent(a.x, a.y, a.button)]\n",
    "pyaction(a) = a\n",
    "\n",
    "function Reinforce.step!(env::GymEnv, s, a)\n",
    "    # info(\"Going to take action: $a\")\n",
    "    pyact = pyaction(a)\n",
    "    s′, r, env.done, env.info = env.pyenv[:step](pyact)\n",
    "    env.reward, env.state = r, s′\n",
    "end\n",
    "\n",
    "function Reinforce.step!(env::UniverseEnv, s, a)\n",
    "    info(\"Going to take action: $a\")\n",
    "    pyact = Any[pyaction(a)]\n",
    "    s′, r, env.done, env.info = env.pyenv[:step](pyact)\n",
    "    env.reward, env.state = r, s′\n",
    "end\n",
    "\n",
    "Reinforce.finished(env::GymEnv, s′) = env.done\n",
    "Reinforce.finished(env::UniverseEnv, s′) = all(env.done)\n",
    "\n",
    "function main()\n",
    "    @static if is_linux()\n",
    "        # due to a ssl library bug, I have to first load the ssl lib here\n",
    "        condadir = Pkg.dir(\"Conda\",\"deps\",\"usr\",\"lib\")\n",
    "        Libdl.dlopen(joinpath(condadir, \"libssl.so\"))\n",
    "        Libdl.dlopen(joinpath(condadir, \"python2.7\", \"lib-dynload\", \"_ssl.so\"))\n",
    "    end\n",
    "\n",
    "    global const pygym = pyimport(\"gym\")\n",
    "end\n",
    "\n",
    "main()\n",
    "\n",
    "end # module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 1 finished after ([0.0200146, 0.147416, -0.0103349, -0.25302], 1, 1.0, [0.022963, 0.342684, -0.0153953, -0.548945]) steps. Total reward: ([0.020972, -0.0478652, -0.0111984, 0.0431747], 1, 1.0, [0.0200146, 0.147416, -0.0103349, -0.25302])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 2 finished after ([0.0386194, 0.170186, -0.0248196, -0.288056], 0, 1.0, [0.0420232, -0.0245738, -0.0305807, -0.00330299]) steps. Total reward: ([0.0391252, -0.0252867, -0.0250682, 0.0124298], 1, 1.0, [0.0386194, 0.170186, -0.0248196, -0.288056])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 3 finished after ([0.0329493, 0.161036, -0.0411905, -0.331779], 1, 1.0, [0.03617, 0.356719, -0.0478261, -0.637162]) steps. Total reward: ([0.0336422, -0.0346448, -0.0406595, -0.0265502], 1, 1.0, [0.0329493, 0.161036, -0.0411905, -0.331779])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 4 finished after ([-0.0468723, 0.175824, -0.0155989, -0.321003], 0, 1.0, [-0.0433558, -0.0190726, -0.0220189, -0.0332799]) steps. Total reward: ([-0.0464821, -0.0195118, -0.0151271, -0.023586], 1, 1.0, [-0.0468723, 0.175824, -0.0155989, -0.321003])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 5 finished after ([-0.0232148, -0.235599, 0.0422912, 0.275895], 1, 1.0, [-0.0279268, -0.0411048, 0.0478091, -0.00315479]) steps. Total reward: ([-0.0224171, -0.0398887, 0.0428913, -0.0300059], 0, 1.0, [-0.0232148, -0.235599, 0.0422912, 0.275895])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 6 finished after ([0.0357513, 0.238366, -0.0139092, -0.296453], 1, 1.0, [0.0405186, 0.433683, -0.0198382, -0.59349]) steps. Total reward: ([0.0348903, 0.043047, -0.0139209, 0.000589651], 1, 1.0, [0.0357513, 0.238366, -0.0139092, -0.296453])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 7 finished after ([-0.0177162, 0.218851, 0.0389537, -0.238447], 0, 1.0, [-0.0133392, 0.0231947, 0.0341848, 0.0662638]) steps. Total reward: ([-0.0182021, 0.0242956, 0.0381143, 0.0419707], 1, 1.0, [-0.0177162, 0.218851, 0.0389537, -0.238447])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 8 finished after ([-0.0180853, 0.176264, 0.00985666, -0.266444], 0, 1.0, [-0.0145601, -0.0189976, 0.00452778, 0.029331]) steps. Total reward: ([-0.0177109, -0.0187223, 0.00939145, 0.0232607], 1, 1.0, [-0.0180853, 0.176264, 0.00985666, -0.266444])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 9 finished after ([-0.0436506, 0.180154, 0.018225, -0.285191], 0, 1.0, [-0.0400476, -0.0152229, 0.0125212, 0.0131835]) steps. Total reward: ([-0.0433566, -0.0147023, 0.0181911, 0.00169702], 1, 1.0, [-0.0436506, 0.180154, 0.018225, -0.285191])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 10 finished after ([-0.0418147, 0.208142, 0.0403405, -0.238303], 1, 1.0, [-0.0376519, 0.402665, 0.0355745, -0.517993]) steps. Total reward: ([-0.0420869, 0.0136082, 0.0395074, 0.0416582], 1, 1.0, [-0.0418147, 0.208142, 0.0403405, -0.238303])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 11 finished after ([0.00588525, -0.190398, -0.00130981, 0.290954], 1, 1.0, [0.00207729, 0.00474227, 0.00450928, -0.00214125]) steps. Total reward: ([0.00579115, 0.00470519, -0.00128335, -0.00132325], 0, 1.0, [0.00588525, -0.190398, -0.00130981, 0.290954])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 12 finished after ([0.0306574, -0.215262, 0.0369469, 0.298607], 0, 1.0, [0.0263522, -0.410891, 0.0429191, 0.60271]) steps. Total reward: ([0.03105, -0.0196287, 0.0370576, -0.00553365], 0, 1.0, [0.0306574, -0.215262, 0.0369469, 0.298607])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 13 finished after ([-0.0457334, 0.240831, 0.0283335, -0.29701], 1, 1.0, [-0.0409167, 0.435538, 0.0223933, -0.580624]) steps. Total reward: ([-0.046656, 0.0461307, 0.0286032, -0.013487], 1, 1.0, [-0.0457334, 0.240831, 0.0283335, -0.29701])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 14 finished after ([0.0463149, 0.189851, -0.0501084, -0.336331], 0, 1.0, [0.0501119, -0.00452301, -0.056835, -0.0598607]) steps. Total reward: ([0.0464338, -0.00594473, -0.0495396, -0.0284381], 1, 1.0, [0.0463149, 0.189851, -0.0501084, -0.336331])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 15 finished after ([0.0327208, -0.166375, 0.0316547, 0.344377], 1, 1.0, [0.0293933, 0.028283, 0.0385422, 0.0618419]) steps. Total reward: ([0.0321372, 0.0291753, 0.030812, 0.0421343], 0, 1.0, [0.0327208, -0.166375, 0.0316547, 0.344377])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 16 finished after ([-0.0431544, -0.180526, -0.0134174, 0.287998], 0, 1.0, [-0.0467649, -0.375454, -0.00765744, 0.576419]) steps. Total reward: ([-0.0434424, 0.0144008, -0.0134089, -0.000424592], 0, 1.0, [-0.0431544, -0.180526, -0.0134174, 0.287998])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 17 finished after ([-0.00333569, 0.22371, -0.0227513, -0.320832], 0, 1.0, [0.00113851, 0.0289195, -0.0291679, -0.0354096]) steps. Total reward: ([-0.0039012, 0.0282753, -0.0223275, -0.0211887], 1, 1.0, [-0.00333569, 0.22371, -0.0227513, -0.320832])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 18 finished after ([-0.00487354, -0.16733, 0.00250592, 0.259552], 1, 1.0, [-0.00822014, 0.027756, 0.00769695, -0.0323399]) steps. Total reward: ([-0.00543029, 0.0278375, 0.00318863, -0.0341357], 0, 1.0, [-0.00487354, -0.16733, 0.00250592, 0.259552])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 19 finished after ([0.0207911, 0.214106, -0.0143385, -0.27585], 0, 1.0, [0.0250732, 0.0191912, -0.0198555, 0.0122762]) steps. Total reward: ([0.0204156, 0.0187751, -0.0147676, 0.0214554], 1, 1.0, [0.0207911, 0.214106, -0.0143385, -0.27585])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 20 finished after ([0.00889072, 0.161972, -0.0326623, -0.340771], 1, 1.0, [0.0121302, 0.357543, -0.0394777, -0.643573]) steps. Total reward: ([0.00956257, -0.0335926, -0.0318983, -0.0381974], 1, 1.0, [0.00889072, 0.161972, -0.0326623, -0.340771])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 21 finished after ([-0.0385302, -0.229276, 0.0198754, 0.262165], 1, 1.0, [-0.0431157, -0.0344438, 0.0251187, -0.0241835]) steps. Total reward: ([-0.0378528, -0.0338651, 0.0206144, -0.0369503], 0, 1.0, [-0.0385302, -0.229276, 0.0198754, 0.262165])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 22 finished after ([0.00587435, 0.186746, 0.0203428, -0.325364], 0, 1.0, [0.00960927, -0.00865952, 0.0138355, -0.0263364]) steps. Total reward: ([0.00603568, -0.00806658, 0.0211313, -0.0394229], 1, 1.0, [0.00587435, 0.186746, 0.0203428, -0.325364])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 23 finished after ([-0.0165084, 0.224322, -0.0380352, -0.323851], 1, 1.0, [-0.0120219, 0.419965, -0.0445122, -0.628282]) steps. Total reward: ([-0.017082, 0.0286814, -0.0376446, -0.0195328], 1, 1.0, [-0.0165084, 0.224322, -0.0380352, -0.323851])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 24 finished after ([-0.00195924, -0.208308, -0.0500984, 0.231863], 0, 1.0, [-0.0061254, -0.40268, -0.0454611, 0.508332]) steps. Total reward: ([-0.00168075, -0.0139249, -0.0492004, -0.0448999], 0, 1.0, [-0.00195924, -0.208308, -0.0500984, 0.231863])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 25 finished after ([0.0410059, 0.203412, 0.0327838, -0.279098], 1, 1.0, [0.0450742, 0.398051, 0.0272018, -0.561263]) steps. Total reward: ([0.0408304, 0.00877406, 0.0327221, 0.00308374], 1, 1.0, [0.0410059, 0.203412, 0.0327838, -0.279098])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 26 finished after ([-0.00772924, 0.212124, 0.0227283, -0.30283], 1, 1.0, [-0.00348677, 0.406914, 0.0166717, -0.58826]) steps. Total reward: ([-0.00807605, 0.0173402, 0.0230786, -0.0175175], 1, 1.0, [-0.00772924, 0.212124, 0.0227283, -0.30283])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 27 finished after ([0.00731412, 0.161285, -0.00443237, -0.254356], 1, 1.0, [0.0105398, 0.35647, -0.0095195, -0.548434]) steps. Total reward: ([0.00799235, -0.0339116, -0.00523183, 0.0399729], 1, 1.0, [0.00731412, 0.161285, -0.00443237, -0.254356])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 28 finished after ([0.0410935, -0.23822, 0.000975024, 0.325745], 0, 1.0, [0.0363291, -0.433356, 0.00748992, 0.618735]) steps. Total reward: ([0.0419554, -0.0430936, 0.000315774, 0.0329625], 0, 1.0, [0.0410935, -0.23822, 0.000975024, 0.325745])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 29 finished after ([-0.0263578, -0.171167, -0.0231747, 0.329454], 1, 1.0, [-0.0297811, 0.0242772, -0.0165857, 0.029554]) steps. Total reward: ([-0.0268298, 0.0236019, -0.0240639, 0.0444597], 0, 1.0, [-0.0263578, -0.171167, -0.0231747, 0.329454])\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mEpisode 30 finished after ([0.0128201, 0.153281, 0.00815777, -0.264429], 0, 1.0, [0.0158857, -0.0419568, 0.00286919, 0.0308155]) steps. Total reward: ([0.0136547, -0.0417309, 0.00764111, 0.025833], 1, 1.0, [0.0128201, 0.153281, 0.00815777, -0.264429])\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "using OpenAIGym\n",
    "ENV[\"PYTHON\"] = \"/Users/manvithaponnapati/gymenvaffinity/bin/python\"\n",
    "\n",
    "env = GymEnv(\"CartPole-v0\")\n",
    "for i=1:30\n",
    "    R, T = Episode(env, RandomPolicy())\n",
    "    env.pyenv[:render]()\n",
    "    info(\"Episode $i finished after $T steps. Total reward: $R\")\n",
    "end\n",
    "#Use this close the window if you are on Julia notebooks\n",
    "env.pyenv[:render](close=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
